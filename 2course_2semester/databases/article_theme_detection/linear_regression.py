from math import exp, log
import numpy as np
import scipy.optimize as op


def sigmoid(z):
    return 1 / (1 + np.exp(-z))


def h(theta, X):
    return sigmoid(X.dot(theta))


def cost(theta, X, Y):
    m = len(Y)
    n = len(theta)
    J = 0
    for i in range(m):
        if Y[i] == 1:
            s = sigmoid(X[i].dot(theta))
            if s == 0:
                J += 99999999999
            else:
                J -= log(s)
        else:
            s = 1 - sigmoid(X[i].dot(theta))
            if s == 0:
                J += 99999999999
            else:
                J -= log(s)
    J /= m
    return J


def derivatives(theta, X, Y):
    m = len(Y)
    grad = (h(theta, X) - Y).transpose().dot(X).transpose() / m
    return grad.flatten()


def gradient_descent(X, Y):
    m = len(X)
    n = len(X[0])

    theta = np.array([0] * n)
    #theta = [0.05748704974664037, -0.047570220783230484, -0.07827155544591649, -0.08610025373315629, -0.08624940960010782, -0.01704478867763585, -0.13792449781242974, -0.07013245098494202, -0.047334060229102165, -0.07163646735157035, -0.062300645204941135, -0.023339555594016098, -0.04492088422483184, -0.05648186434605709, 0.0046938376387332635, 0.005210802588930132, -0.02453177108516768, 0.010384066868917411, -0.053120094281454165, -0.05642358030343633, -0.04301216914628907, -0.014141079480783867, -0.02037745468501175, -0.05393431176414977, -0.025233948212631144, -0.04660223366305845, -0.045413128389343665, -0.023339555594016098, -0.033180739953566143, -0.030743214986879014, -0.0358750922988307, -0.03986313633976162, -0.045967433913351355, 0.014969784416663743, -0.03715493675574016, -0.05549269143784795, -0.04594234846791043, -0.015193377762886482, -0.020123721947460056, -0.05532373973585795, -0.04086242192984911, -0.03329755646880512, -0.018886046538104617, -0.05300089411737822, -0.033192686963021335, -0.034786589488009806, -0.048472747807445496, -0.043586821443564176, -0.031147431740108037, -0.03775960887111622, -0.046226530556747185, -0.022510709383968416, -0.03889637846689928, -0.032204778992952356, -0.04263010282625298, -0.03809740649962923, -0.025760484459778292, -0.032901833451159185, -0.04992794357032852, -0.022335956524438774, -0.028285746526781272, -0.03429301401753339, -0.030215075931067852, -0.04134720474659416, 0.017412512468127455, -0.034099480635155575, -0.01953668861738219, -0.03114581512666656, -0.025105244909683036, -0.027656169019301666, -0.019760310276971928, -0.038432825002649156, -0.02270885860772306, -0.020001064662179168, -0.02853422523522573, -0.039826938718254164, -0.06332064280960391, -0.05272427941191741, -0.022817614143949772, -0.013558149597724622, -0.04571243118405172, -0.01858539912334033, -0.02169202402849552, -0.0623301495962795, -0.03649120145016717, -0.0221307132441528, -0.029377468790824476, -0.008588833694829632, -0.010472616801082881, -0.02279087334437164, -0.010799780399203183, -0.05104571291720572, -0.046206625421081844, -0.018241580396322208, -0.020079463766453472, -0.029043218556970812, -0.020020374910538166, -0.015046017378649955, -0.036840310618955416, 0.0, 0.0, -0.03526679954121711, -0.03526679954121711, -0.019470230788948852, 0.0, 0.0, -0.004833456863565897, -0.01296398339183295, -0.02421977265033274, -0.02906606989273317, -0.019563613223642442, -0.038980402150149404, -0.01189939844976091, -0.03167403445562555, -0.027233269653046482, -0.045556163587121654, -0.0167720406322164, -0.03603103633358746, -0.0530572931020983, -0.03552583467887921, -0.03136421248614029, -0.01920795945368035, -0.052455945607150514, -0.025224665225247724, -0.011462511962678687, -0.032377403356282745, 0.0, 0.0012999619520434123, -0.021332795876388096, -0.04439041800405362, -0.022449909048901144, -0.020640506934498704, -0.027561044260758373, -0.018474482947090354, -0.03985711774424879, -0.03527237159852373, -0.01579629253155482, -0.0034014709025829344, -0.018677480846699516, -0.021225110584440116, -0.020892464104726477, -0.019927960480523425, -0.046394111158279745, 0.013463461368317785, -0.021801959805550022, -0.029877537542514383, -0.010104353069117984, -0.025047458785702373, -0.03648941278304712, -0.015633134644422068, -0.022316837453033322, 0.1114515876537208, 0.06466142295522363, 0.060394560728978176, 0.06902552234909466, 0.038772141287155955, 0.04567696700089124, 0.0607955943329864, 0.014245228122124807, 0.03737668251792076, 0.08037810923359642, 0.043603821366974285, 0.059613411097722654, 0.04011725037118865, 0.06285194083316957, 0.01372299833435311, 0.06519836280266189, 0.04910140414671928, 0.01841553877458612, 0.03251322134958363, -0.007316195964264125, 0.02311068780466163, -0.011277759899619216, 0.022732073794549508, 0.031236717814926023, 0.04625629155851025, 0.008736732597806288, 0.018343424403626105, 0.0, 0.024278817833323467, 0.026909995118037072, 0.013130631477484899, 0.0345326280474309, 0.05256231987669621, 0.009113559890335843, 0.05385139815769318, 0.0172423845184596, 0.020340814569774216, 0.04404281354511407, 0.01616354592017481, 0.009815203955624305, 0.017749015560084146, -0.0039460989489542376, 0.015861326673062698, 0.017003443663599207, 0.01142595867939735, 0.015379312065761293, 0.02580402405376598, 0.021985735848580792, 0.02486553308596397, 0.025374612944370087, 0.007214781248209538, 0.028156035164396336, 0.000648540277235952, 0.015238196551669312, -0.009117238923267256, 0.009306071012152679, 0.015620805921750757, 0.014968506958978242, 0.03451824201306451, -0.014733408317556191, 0.019227147453083335, -0.0022401309632874445, 0.021906481392994285, 0.01563844743309845, -0.005331440199616948, 0.01875885633877395, 0.01945468022978456, 0.015424976494440157, 0.022801400223373035, 0.0009238688079995491, -0.0029387676087179165, 0.06914412086949778, 0.01836544529911909, 0.011359129533103541, 0.020808254781682734, 0.04168573253396696, -0.006502669174919764, 0.01294891014332575, 0.020985448615166893, 0.03382675985168398, 0.004387482168575528, 0.009567058441248302, 0.0489551320426267, 0.01883377874763977, 0.03885365127155366, 0.0034832642575555388, 0.009101992526552048, 0.018146878708290883, 0.006796172427518108, 0.03479375898175646, 0.023775143230043254, 0.02050968589965074, 0.012831580563057067, 0.005049979309936495, 0.012633502852451309, 0.014143121378397312, 0.019655018720176165, 0.0648543411725898, 0.02742842480745465, 0.022375313779534115, 0.029831917443749325, 0.006352922097984561, 0.013306798887362394, 0.0018269648437708901, 0.02071209263894223, 0.015566057602911998, -0.015416914540387917, 0.023321772619357778, 0.01735719559866937, 0.005754933274158685, 0.0173176228189588, 0.004717764216508591, 0.013724855484763187, 0.003929384443998781, 0.010256885557517957, 0.003815999364077626, 0.025998987194669813, 0.020233455267062186, 0.016946522948527023, 0.013474941523188172, 0.02819769802347745, -0.003318371084195993, 0.019280859298531704, 0.003554660544331028, 0.008792045061122583]
    alpha = 0.01

    for i in range(100):
        j = J(theta, X, Y)
        print("Iteration #" + str(i), "cost function:", j)

        d = derivatives(theta, X, Y)
        #for k in range(n):
        #    theta[k] -= alpha * d[k]
        theta = theta - alpha * d

    return theta


def read_training_set():
    f = open("training_set.txt", "r")
    numbers = [line.split() for line in f.readlines()]
    m = len(numbers)
    n = len(numbers[0])
    X = []
    Y = []
    for i in range(m):
        X.append([])
        for j in range(n - 1):
            X[i].append(int(numbers[i][j]))
        Y.append(int(numbers[i][-1]))

    X = np.array(X)
    Y = np.array(Y)
    return X, Y


print("Reading training set...")
X, Y = read_training_set()
print("Done")

print("Starting gradient descent")
theta = [0] * len(X[0])#gradient_descent(X, Y)
#theta = scipy.optimize.minimize(J, theta, args=(X, Y), method="SLSQP", jac=derivatives, options={"disp": True})
#theta = gradient_descent(X, Y)

result = op.minimize(fun=cost, x0=theta, args=(X, Y), method="TNC", jac=derivatives, options={"disp": True})
theta = result.x

print("Done")

print()
#print("Theta:", theta)
#print(h(theta, X) > 0.5)

print("Cost function:", cost(theta, X, Y))

m = len(Y)
correct = 0

for i in range(m):
    p = h(theta, X[i])
    if p >= 0.5 and Y[i] == 1:
        correct += 1
    elif p < 0.5 and Y[i] == 0:
        correct += 1

print("Accuracy: ", correct / m * 100, "%", sep="")

f = open("theta.txt", "w")
for t in theta:
    print(t, file=f)
f.close()